{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpUGUF3eV4CG",
        "outputId": "7f9e9bc5-34a7-477c-c6fc-f934b7523afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyMuPDFb==1.23.7 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=d0be9cffe62885383e03f4b137fa31d638fa3a35507bc03c4371cccce2a4bbe4\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, faiss-cpu, PyMuPDFb, pymupdf, sentence-transformers\n",
            "Successfully installed PyMuPDFb-1.23.7 faiss-cpu-1.7.4 pymupdf-1.23.7 sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf transformers faiss-cpu torch sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1**This line installs the required Python packages using the pip package manager.**\n"
      ],
      "metadata": {
        "id": "L6i2fzaVZI3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "Z0aHUPzoV49w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2**These lines import necessary libraries and modules for working with PDFs, numerical operations, Faiss for similarity search, PyTorch for deep learning, and various models for natural language processing.**"
      ],
      "metadata": {
        "id": "j012DFYxZO9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "5FWLqylIWEY-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3**Defines a function extract_text_from_pdf that takes a PDF file path as input and returns the extracted text using the PyMuPDF library (fitz).**"
      ],
      "metadata": {
        "id": "dP1TS8MSZWJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Transformer\n",
        "tokenizer_hf = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model_hf = AutoModel.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "tJSOjS0kWG1o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4**Initializes a tokenizer and model from the Hugging Face Transformers library using the BERT model architecture.**"
      ],
      "metadata": {
        "id": "xej0lGP0Zdfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_hf_embeddings(text):\n",
        "    inputs = tokenizer_hf(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_hf(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "4-35j5TdWKBQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5**Defines a function create_hf_embeddings that takes a text as input, tokenizes it, and obtains embeddings using the Hugging Face model. The embeddings are the mean of the last hidden states.**"
      ],
      "metadata": {
        "id": "j6nhqGVvZgxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Transformer\n",
        "model_st = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "INw_zGHEWPSO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6**Initializes a Sentence Transformer model using the MiniLM architecture.**\n"
      ],
      "metadata": {
        "id": "-VQjRf0bZpw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_st_embeddings(text):\n",
        "    return model_st.encode(text)\n"
      ],
      "metadata": {
        "id": "6Oyi8h8FWUBw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7**Defines a function create_st_embeddings that takes a text as input and obtains embeddings using the Sentence Transformer model.**"
      ],
      "metadata": {
        "id": "tLdq-TcNZvFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = [\n",
        "    \"/content/sample1.pdf\",\n",
        "    \"/content/sample2.pdf\",\n",
        "    \"/content/sample3.pdf\",\n",
        "    \"/content/sample4.pdf\",\n",
        "    \"/content/sample5.pdf\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "FQnhJ26jWY1J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8**Defines a list of file paths for PDF documents.**\n"
      ],
      "metadata": {
        "id": "jnoNhR1WZ1WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [extract_text_from_pdf(pdf) for pdf in pdf_paths]\n",
        "embeddings_hf = [create_hf_embeddings(text) for text in texts]\n",
        "embeddings_st = [create_st_embeddings(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "79iLFDaCWcIa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9**Extracts text from each PDF and computes embeddings using both Hugging Face and Sentence Transformer models.**"
      ],
      "metadata": {
        "id": "4sKsIWT9Z6bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_flat_hf = np.vstack(embeddings_hf)\n",
        "embeddings_flat_st = np.vstack(embeddings_st)\n"
      ],
      "metadata": {
        "id": "DgZLTR12Wers"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10**Stacks the embeddings vertically to create matrices of embeddings for each model**"
      ],
      "metadata": {
        "id": "HwXBu2huaAK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(embeddings, file_name):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    faiss.write_index(index, file_name)\n"
      ],
      "metadata": {
        "id": "AogcQwieWhKW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11**Defines a function save_embeddings that takes embeddings and a file name as input, creates a Faiss index, adds embeddings to the index, and writes the index to a file.**"
      ],
      "metadata": {
        "id": "y8e9aggRaEfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_embeddings(embeddings_flat_hf, \"embeddings_hf.index\")\n",
        "save_embeddings(embeddings_flat_st, \"embeddings_st.index\")\n"
      ],
      "metadata": {
        "id": "y0ObUkMNWj9u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11**Saves the embeddings to Faiss index files.**\n"
      ],
      "metadata": {
        "id": "I3gclA7jaMw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_index(file_name):\n",
        "    return faiss.read_index(file_name)\n"
      ],
      "metadata": {
        "id": "2kvMYARFWmLs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12**Defines a function load_index that reads a Faiss index from a file.**\n"
      ],
      "metadata": {
        "id": "M_Wd4-x2aQyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_index(index, embedding):\n",
        "    _, I = index.search(embedding, k=1)\n",
        "    return I[0][0]\n"
      ],
      "metadata": {
        "id": "NwQEgkRnWoti"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13**Defines a function search_index that takes a Faiss index and an embedding, performs a similarity search, and returns the index of the nearest neighbor.**\n"
      ],
      "metadata": {
        "id": "FT0KMuWYaWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, texts, model, tokenizer=None):\n",
        "    if tokenizer:\n",
        "        embedding = create_hf_embeddings(question)\n",
        "    else:\n",
        "        embedding = model.encode(question)\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "    idx = search_index(load_index(\"embeddings_hf.index\" if tokenizer else \"embeddings_st.index\"), embedding)\n",
        "    return texts[idx]\n"
      ],
      "metadata": {
        "id": "y4fwKo8uWq18"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14**Defines a function answer_question that takes a question, a list of texts, a model, and an optional tokenizer. It computes the embedding of the question using either Hugging Face or Sentence Transformer, performs a similarity search, and returns the answer.**\n"
      ],
      "metadata": {
        "id": "pQqP283raciW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Outline the key tenets of sustainable development.\"\n",
        "\"Explain the functioning of quantum computing.\"\n",
        "\"Provide a historical overview of the Roman Empire.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "m0RWAJY6Wsxh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15**Defines a list of example questions.**"
      ],
      "metadata": {
        "id": "3d7_CRWnah8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare answers from both models\n",
        "for question in questions:\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"Answer using Hugging Face model:\", answer_question(question, texts, model_hf, tokenizer_hf))\n",
        "    print(\"Answer using Sentence Transformer model:\", answer_question(question, texts, model_st))\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoXg1eELWu07",
        "outputId": "65f43b4f-0551-445b-a8fb-544640c8e3d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Outline the key tenets of sustainable development.Explain the functioning of quantum computing.Provide a historical overview of the Roman Empire.\n",
            "Answer using Hugging Face model: Description: Consult for laparoscopic gastric bypass.. \n",
            "medical_specialty: Bariatrics \n",
            "sample_name : Laparoscopic Gastric Bypass Consult - 1  \n",
            "transcription:  \n",
            "HISTORY OF PRESENT ILLNESS: , I have seen ABC today.  He is a very pleasant gentleman who is 42 \n",
            "years old, 344 pounds.  He is 5'9\".  He has a BMI of 51.  He has been overweight for ten years since \n",
            "the age of 33, at his highest he was 358 pounds, at his lowest 260.  He is pursuing surgical attempts \n",
            "of weight loss to feel good, get healthy, and begin to exercise again.  He wants to be able to exercise \n",
            "and play volleyball.  Physically, he is sluggish.  He gets tired quickly.  He does not go out often.  When \n",
            "he loses weight he always regains it and he gains back more than he lost.  His biggest weight loss is \n",
            "25 pounds and it was three months before he gained it back.  He did six months of not drinking \n",
            "alcohol and not taking in many calories.  He has been on multiple commercial weight loss programs \n",
            "including Slim Fast for one month one year ago and Atkin's Diet for one month two years ago., \n",
            " \n",
            "PAST MEDICAL HISTORY: , He has difficulty climbing stairs, difficulty with airline seats, tying shoes, \n",
            "used to public seating, difficulty walking, high cholesterol, and high blood pressure.  He has asthma \n",
            "and difficulty walking two blocks or going eight to ten steps.  He has sleep apnea and snoring.  He is a \n",
            "diabetic, on medication.  He has joint pain, knee pain, back pain, foot and ankle pain, leg and foot \n",
            "swelling.  He has hemorrhoids., \n",
            " \n",
            "PAST SURGICAL HISTORY: , Includes orthopedic or knee surgery.,SOCIAL HISTORY: , He is currently \n",
            "single.  He drinks alcohol ten to twelve drinks a week, but does not drink five days a week and then \n",
            "will binge drink.  He smokes one and a half pack a day for 15 years, but he has recently stopped \n",
            "smoking for the past two weeks.,FAMILY HISTORY: , Obesity, heart disease, and diabetes.  Family \n",
            "history is negative for hypertension and stroke., \n",
            " \n",
            "CURRENT MEDICATIONS:,  Include Diovan, Crestor, and Tricor.,MISCELLANEOUS/EATING HISTORY:  \n",
            ",He says a couple of friends of his have had heart attacks and have had died.  He used to drink \n",
            "everyday, but stopped two years ago.  He now only drinks on weekends.  He is on his second week of \n",
            "Chantix, which is a medication to come off smoking completely.  Eating, he eats bad food.  He is \n",
            "single.  He eats things like bacon, eggs, and cheese, cheeseburgers, fast food, eats four times a day, \n",
            "seven in the morning, at noon, 9 p.m., and 2 a.m.  He currently weighs 344 pounds and 5'9\".  His \n",
            "ideal body weight is 160 pounds.  He is 184 pounds overweight.  If he lost 70% of his excess body \n",
            "weight that would be 129 pounds and that would get him down to 215., \n",
            " \n",
            "REVIEW OF SYSTEMS: , Negative for head, neck, heart, lungs, GI, GU, orthopedic, or skin.  He also is \n",
            "positive for gout.  He denies chest pain, heart attack, coronary artery disease, congestive heart \n",
            "failure, arrhythmia, atrial fibrillation, pacemaker, pulmonary embolism, or CVA.  He denies venous \n",
            "insufficiency or thrombophlebitis.  Denies shortness of breath, COPD, or emphysema.  Denies \n",
            "thyroid problems, hip pain, osteoarthritis, rheumatoid arthritis, GERD, hiatal hernia, peptic ulcer \n",
            "disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, rectal bleeding, polyps, \n",
            "incontinence of stool, urinary stress incontinence, or cancer.  He denies cellulitis, pseudotumor \n",
            "cerebri, meningitis, or encephalitis., \n",
            " \n",
            "PHYSICAL EXAMINATION:  ,He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Neck is soft \n",
            "and supple.  Lungs:  He has positive wheezing bilaterally.  Heart is regular rhythm and rate.  His \n",
            "abdomen is soft.  Extremities:  He has 1+ pitting edema.,IMPRESSION/PLAN:,  I have explained to him \n",
            "the risks and potential complications of laparoscopic gastric bypass in detail and these include \n",
            "bleeding, infection, deep venous thrombosis, pulmonary embolism, leakage from the gastrojejuno-\n",
            "anastomosis, jejunojejuno-anastomosis, and possible bowel obstruction among other potential \n",
            "complications.  He understands.  He wants to proceed with workup and evaluation for laparoscopic \n",
            "Roux-en-Y gastric bypass.  He will need to get a letter of approval from Dr. XYZ.  He will need to see a \n",
            "nutritionist and mental health worker.  He will need an upper endoscopy by either Dr. XYZ.  He will \n",
            "need to go to Dr. XYZ as he previously had a sleep study.  We will need another sleep study.  He will \n",
            "need H. pylori testing, thyroid function tests, LFTs, glycosylated hemoglobin, and fasting blood sugar.  \n",
            "After this is performed, we will submit him for insurance approval. \n",
            " \n",
            " \n",
            "Keywords: bariatrics, laparoscopic gastric bypass, heart attacks, body weight, pulmonary embolism, \n",
            "potential complications, sleep study, weight loss, gastric bypass, anastomosis, loss, sleep, \n",
            "laparoscopic, gastric, bypass, heart, pounds, weight, \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "Answer using Sentence Transformer model: Description: 2-D M-Mode. Doppler.   \n",
            "medical_specialty: Cardiovascular / Pulmonary \n",
            "sample_name : 2-D Echocardiogram - 1  \n",
            "transcription:  \n",
            "2-D M-MODE: , ,1.  Left atrial enlargement with left atrial diameter of 4.7 cm.,2.  Normal size right \n",
            "and left ventricle.,3.  Normal LV systolic function with left ventricular ejection fraction of 51%.,4.  \n",
            "Normal LV diastolic function.,5.  No pericardial effusion.,6.  Normal morphology of aortic valve, \n",
            "mitral valve, tricuspid valve, and pulmonary valve.,7.  PA systolic pressure is 36 mmHg.,DOPPLER: , \n",
            ",1.  Mild mitral and tricuspid regurgitation.,2.  Trace aortic and pulmonary regurgitation. \n",
            " \n",
            " \n",
            "Keywords: cardiovascular / pulmonary, 2-d m-mode, doppler, aortic valve, atrial enlargement, \n",
            "diastolic function, ejection fraction, mitral, mitral valve, pericardial effusion, pulmonary valve, \n",
            "regurgitation, systolic function, tricuspid, tricuspid valve, normal lv  \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}